wandb_project: tinystories-1m_play
wandb_run_name: null
wandb_run_name_prefix: ""

seed: 0
tlens_model_name: roneneldan/TinyStories-1M
tlens_model_path: null

n_samples: 400_000  # 918604 samples of 512 tokens
save_every_n_samples: null
eval_every_n_samples: 40_000
eval_n_samples: 500
log_every_n_grad_steps: 20
collect_act_frequency_every_n_samples: 40_000
act_frequency_n_tokens: 500_000  #500k tokens is ~977 samples
batch_size: 20
effective_batch_size: 20
lr: 1e-3
lr_schedule: cosine
min_lr_factor: 0.1
warmup_samples: 20_000
max_grad_norm: 1.0

loss:
  sparsity:
    p_norm: 0.0
    initial_coeff: 0.1
    final_coeff: 0.1
    coeff_annealing_schedule: linear
  in_to_orig: null
  out_to_orig: null
  out_to_in:
    coeff: 1.0
  logits_kl: null
train_data:
  dataset_name: apollo-research/roneneldan-TinyStories-tokenizer-gpt2
  is_tokenized: true
  tokenizer_name: gpt2
  streaming: true
  split: train
  n_ctx: 512
eval_data:
  dataset_name: apollo-research/roneneldan-TinyStories-tokenizer-gpt2
  is_tokenized: true
  tokenizer_name: gpt2
  streaming: true
  split: validation
  n_ctx: 512
saes:
  retrain_saes: false
  pretrained_sae_paths: null
  sae_positions:
    - blocks.2.hook_resid_pre
    - blocks.4.hook_resid_pre
    - blocks.6.hook_resid_pre
    - blocks.8.hook_resid_pre
    - blocks.10.hook_resid_pre
  dict_size_to_input_ratio: 50.0
  initial_beta: 5.0
  final_beta: 0.1
  hard_concrete_stretch_limits: [-0.1, 1.1]
